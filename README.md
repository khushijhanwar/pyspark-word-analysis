# pyspark-word-analysis
Word frequency analysis of scientific text using PySpark RDDs and MapReduce transformations. Demonstrates distributed data processing using Python and Apache Spark.
# Analyzing Data with PySpark ğŸ”¥ğŸ“Š

This project uses **Apache PySpark** to perform text-based analysis on the scientific document *"Scientific Feeding"* by Dora C. C. L. Roper. It demonstrates how to process large-scale text data using the RDD API and MapReduce-style transformations.

---

## ğŸ“ File

- `Analyzing_data_with_PySpark.ipynb` â€” Jupyter Notebook performing word frequency analysis using PySpark.

---

## ğŸ” Project Highlights

- Utilizes PySparkâ€™s `RDD` for distributed text processing.
- Applies transformations: `map`, `flatMap`, `reduceByKey`, and `filter`.
- Computes and visualizes top word frequencies.
- Demonstrates the power of scalable big data pipelines in Python.

---

## ğŸ›  Requirements

Make sure you have the following installed:
- Python â‰¥ 3.6  
- Apache Spark  
- Jupyter Notebook  
- PySpark

Install dependencies:
```bash
pip install pyspark notebook matplotlib
