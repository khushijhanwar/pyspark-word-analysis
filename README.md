# pyspark-word-analysis
Word frequency analysis of scientific text using PySpark RDDs and MapReduce transformations. Demonstrates distributed data processing using Python and Apache Spark.
# Analyzing Data with PySpark 🔥📊

This project uses **Apache PySpark** to perform text-based analysis on the scientific document *"Scientific Feeding"* by Dora C. C. L. Roper. It demonstrates how to process large-scale text data using the RDD API and MapReduce-style transformations.

---

## 📁 File

- `Analyzing_data_with_PySpark.ipynb` — Jupyter Notebook performing word frequency analysis using PySpark.

---

## 🔍 Project Highlights

- Utilizes PySpark’s `RDD` for distributed text processing.
- Applies transformations: `map`, `flatMap`, `reduceByKey`, and `filter`.
- Computes and visualizes top word frequencies.
- Demonstrates the power of scalable big data pipelines in Python.

---

## 🛠 Requirements

Make sure you have the following installed:
- Python ≥ 3.6  
- Apache Spark  
- Jupyter Notebook  
- PySpark

Install dependencies:
```bash
pip install pyspark notebook matplotlib
